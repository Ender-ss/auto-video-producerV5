"""
üìù Script Generation Routes
Rotas para gera√ß√£o de roteiros com pipeline de 3 prompts
"""

from flask import Blueprint, request, jsonify
import requests
import json
import time
from services.title_generator import TitleGenerator

scripts_bp = Blueprint('scripts', __name__)

@scripts_bp.route('/generate', methods=['POST'])
def generate_long_script(update_callback=None):
    """Gerar roteiros usando pipeline de 3 prompts"""
    try:
        data = request.get_json()
        title = data.get('title', '')
        premise = data.get('premise', '')
        ai_provider = data.get('ai_provider', 'auto')
        openrouter_model = data.get('openrouter_model', 'auto')
        number_of_chapters = data.get('number_of_chapters', 8)
        api_keys = data.get('api_keys', {})
        
        # Carregar chaves de API do arquivo se n√£o fornecidas
        if not api_keys:
            import os
            config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'api_keys.json')
            if os.path.exists(config_path):
                with open(config_path, 'r') as f:
                    api_keys = json.load(f)

        if not title or not premise:
            return jsonify({
                'success': False,
                'error': 'T√≠tulo e premissa s√£o obrigat√≥rios'
            }), 400

        # Inicializar o gerador
        title_generator = TitleGenerator()
        
        # Configurar APIs baseado no provider
        if ai_provider == 'openai' or ai_provider == 'auto':
            if api_keys.get('openai'):
                title_generator.configure_openai(api_keys['openai'])
        
        if ai_provider == 'gemini' or ai_provider == 'auto':
            gemini_key = api_keys.get('gemini') or api_keys.get('gemini_1')
            if gemini_key:
                title_generator.configure_gemini(gemini_key)
        
        if ai_provider == 'openrouter' or ai_provider == 'auto':
            if api_keys.get('openrouter'):
                title_generator.configure_openrouter(api_keys['openrouter'])

        # Pipeline de 3 prompts
        print("üé¨ Iniciando pipeline de gera√ß√£o de roteiros...")
        
        # PROMPT 1: Tradu√ß√£o e Contexto
        print("üìù Executando Prompt 1: Tradu√ß√£o e Contexto")
        context_result = execute_prompt_1(title, premise, ai_provider, openrouter_model, api_keys, title_generator)
        
        if not context_result:
            raise Exception("Falha no Prompt 1: Tradu√ß√£o e Contexto")
        
        # PROMPT 2: Estrutura Narrativa
        print("üìñ Executando Prompt 2: Estrutura Narrativa")
        narrative_result = execute_prompt_2(title, context_result, ai_provider, openrouter_model, api_keys, title_generator)
        
        if not narrative_result:
            raise Exception("Falha no Prompt 2: Estrutura Narrativa")
        
        # PROMPT 3: Gera√ß√£o dos Cap√≠tulos
        print(f"‚úçÔ∏è Executando Prompt 3: Gera√ß√£o de {number_of_chapters} Cap√≠tulos")
        chapters = execute_prompt_3(title, context_result, narrative_result, number_of_chapters, ai_provider, openrouter_model, api_keys, title_generator)
        
        if not chapters:
            raise Exception("Falha no Prompt 3: Gera√ß√£o dos Cap√≠tulos")

        # Resultado final
        script_result = {
            'title': title,
            'premise': premise,
            'context': context_result,
            'narrative_structure': narrative_result,
            'chapters': chapters,
            'total_chapters': len(chapters),
            'total_words': sum(len(ch['content'].split()) for ch in chapters if ch.get('content'))
        }

        return jsonify({
            'success': True,
            'scripts': script_result,
            'provider_used': ai_provider,
            'chapters_generated': len(chapters)
        })

    except Exception as e:
        print(f"‚ùå Erro na gera√ß√£o de roteiros: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

def execute_prompt_1(title, premise, ai_provider, openrouter_model, api_keys, title_generator):
    """Prompt 1: Tradu√ß√£o e Contexto"""
    prompt = f"""Por favor, forne√ßa o texto acima em Portugu√™s, utilizando nomes e express√µes comuns entre os falantes de Portugu√™s em diferentes pa√≠ses, adaptado de forma a refletir a cultura compartilhada pelos diversos povos que falam a l√≠ngua. Adapte nomes, locais e refer√™ncias culturais de forma a serem naturais e reconhec√≠veis no idioma Portugu√™s, garantindo que mantenham relev√¢ncia e ressoem com o p√∫blico.

A sa√≠da deve ter o seguinte formato:

{{
    "Contexto": "{premise}"
}}

Certifique-se de que a chave gerada siga o padr√£o exigido."""

    try:
        if ai_provider == 'auto':
            providers = ['openrouter', 'gemini', 'openai']
            for provider in providers:
                try:
                    if provider == 'openrouter' and api_keys.get('openrouter'):
                        return call_openrouter(prompt, openrouter_model, api_keys['openrouter'])
                    elif provider == 'gemini' and title_generator.gemini_model:
                        return call_gemini(prompt, title_generator)
                    elif provider == 'openai' and title_generator.openai_client:
                        return call_openai(prompt, title_generator)
                except Exception as e:
                    print(f"‚ùå Erro com {provider}: {e}")
                    continue
        elif ai_provider == 'openrouter':
            return call_openrouter(prompt, openrouter_model, api_keys['openrouter'])
        elif ai_provider == 'gemini':
            return call_gemini(prompt, title_generator)
        elif ai_provider == 'openai':
            return call_openai(prompt, title_generator)
            
        return None
    except Exception as e:
        print(f"‚ùå Erro no Prompt 1: {e}")
        return None

def execute_prompt_2(title, context, ai_provider, openrouter_model, api_keys, title_generator):
    """Prompt 2: Estrutura Narrativa"""
    prompt = f"""# Objetivo Principal: 

Transformar qualquer tema em um √∫nico prompt narrativo para o primeiro cap√≠tulo, sem finalizar a hist√≥ria. O resultado ser√° um array JSON chamado `Prompts`, que conter√° exatamente 1 objeto, com o campo `prompt`.

## Tema: "{title}"
### Contexto: "{context}"

## Estrutura Base do Prompt  
Cada prompt deve come√ßar com: "Escreva uma hist√≥ria de aproximadamente 500 palavras sobre..."  

## Especifica√ß√µes T√©cnicas do JSON

```json
{{
  "Prompts": [
    {{
      "prompt": "Escreva uma hist√≥ria de aproximadamente 500 palavras sobre..."
    }}
  ]
}}
```

## Requisitos T√©cnicos 

- Apenas um objeto no array `Prompts`  
- Cadeia com aspas duplas  
- Indenta√ß√£o: 2 espa√ßos  
- Sem quebras de linha no conte√∫do de `prompt`  
- JSON v√°lido e com caracteres especiais escapados  

## Estrutura Narrativa Detalhada (Apenas Primeiro Cap√≠tulo)

1. **Apresenta√ß√£o do Protagonista**  
   - Nome e caracter√≠sticas principais  
   - Contexto social/profissional  
   - Estado emocional inicial  

2. **Cen√°rio Principal**  
   - Localiza√ß√£o temporal e espacial  
   - Atmosfera e ambiente  
   - Elementos √∫nicos do mundo da hist√≥ria  

3. **Ativador da Hist√≥ria**  
   - Evento catalisador que rompa a normalidade  
   - Inicie o conflito inicial ou dilema principal  

4. **Gancho Narrativo**  
   - Elemento de mist√©rio ou tens√£o que deixe clara a continua√ß√£o  
   - Pergunta n√£o resolvida  
   - Termine sem concluir o conflito, mantendo a hist√≥ria incompleta 

## Checklist de Valida√ß√£o

1. **JSON**  
   - [ ] Apenas 1 objeto dentro de "Prompts"  
   - [ ] Campo "prompt" sem quebras de linha  
   - [ ] Cadeias com aspas duplas, sem caracteres especiais 

2. **Narrativa**  
   - [ ] Hist√≥ria N√ÉO finalizada (o cap√≠tulo fica em aberto)    
   - [ ] Inclui ganchos, sem resolver o conflito  
   - [ ] N√£o conclua a trama nem resolva o conflito principal. Em vez disso, crie um problema a partir de cada solu√ß√£o, fazendo o leitor sentir que a hist√≥ria ainda precisa continuar. A sensa√ß√£o final deve ser de que h√° mais por vir, nunca uma conclus√£o definitiva.  
   - [ ] Certifique-se de que o cap√≠tulo termine com um gancho intrigante, criando um mist√©rio ou uma d√∫vida que deixe o leitor ansioso pelo pr√≥ximo cap√≠tulo, sem conseguir parar de ler.  
   - [ ] Mantenha um equil√≠brio entre a√ß√£o e reflex√£o, permitindo que o personagem evolua, mas sem dar respostas definitivas ou conclus√µes que resolvam os dilemas abertos.

## Observa√ß√£o Final

- N√£o conclua a trama nem resolva o conflito principal. Em vez disso, crie um problema a partir de cada solu√ß√£o, fazendo o leitor sentir que a hist√≥ria ainda precisa continuar. A sensa√ß√£o final deve ser que h√° mais por vir, nunca uma conclus√£o definitiva.  
- Certifique-se de que o cap√≠tulo termine com um gancho intrigante, criando um mist√©rio ou uma d√∫vida que deixe o leitor ansioso pelo pr√≥ximo cap√≠tulo, sem conseguir parar de ler.  
- Mantenha um equil√≠brio entre a√ß√£o e reflex√£o, permitindo que o personagem evolua, mas sem dar respostas definitivas ou conclus√µes que resolvam os dilemas abertos."""

    try:
        if ai_provider == 'auto':
            providers = ['openrouter', 'gemini', 'openai']
            for provider in providers:
                try:
                    if provider == 'openrouter' and api_keys.get('openrouter'):
                        return call_openrouter(prompt, openrouter_model, api_keys['openrouter'])
                    elif provider == 'gemini' and title_generator.gemini_model:
                        return call_gemini(prompt, title_generator)
                    elif provider == 'openai' and title_generator.openai_client:
                        return call_openai(prompt, title_generator)
                except Exception as e:
                    print(f"‚ùå Erro com {provider}: {e}")
                    continue
        elif ai_provider == 'openrouter':
            return call_openrouter(prompt, openrouter_model, api_keys['openrouter'])
        elif ai_provider == 'gemini':
            return call_gemini(prompt, title_generator)
        elif ai_provider == 'openai':
            return call_openai(prompt, title_generator)
            
        return None
    except Exception as e:
        print(f"‚ùå Erro no Prompt 2: {e}")
        return None

def execute_prompt_3(title, context, narrative_result, number_of_chapters, ai_provider, openrouter_model, api_keys, title_generator, update_callback=None, custom_inicio='', custom_meio='', custom_fim=''):
    """Prompt 3: Gera√ß√£o dos Cap√≠tulos"""
    chapters = []

    try:
        print(f"üîç DEBUG: Iniciando gera√ß√£o de {number_of_chapters} cap√≠tulos")
        print(f"üîç DEBUG: AI Provider: {ai_provider}")
        print(f"üîç DEBUG: APIs dispon√≠veis: {list(api_keys.keys())}")

        # Verificar se h√° pelo menos uma API configurada
        has_openrouter = api_keys.get('openrouter') is not None and title_generator.openrouter_api_key is not None
        has_gemini = title_generator.gemini_model is not None
        has_openai = title_generator.openai_client is not None

        print(f"üîç DEBUG: OpenRouter configurado: {'‚úÖ' if has_openrouter else '‚ùå'}")
        print(f"üîç DEBUG: Gemini configurado: {'‚úÖ' if has_gemini else '‚ùå'}")
        print(f"üîç DEBUG: OpenAI configurado: {'‚úÖ' if has_openai else '‚ùå'}")

        if not (has_openrouter or has_gemini or has_openai):
            print("‚ùå ERRO: Nenhuma API de IA configurada para gera√ß√£o de roteiros")
            return []

        # Extrair o prompt base da estrutura narrativa
        base_prompt = extract_base_prompt(narrative_result)
        print(f"üîç DEBUG: Base prompt extra√≠do: {base_prompt[:100]}...")
        
        # Importar fun√ß√µes auxiliares para prompts de roteiro
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.dirname(__file__)))
        from routes.premise import create_inicio_prompt, create_capitulo_prompt, create_final_prompt
        
        for i in range(number_of_chapters):
            print(f"üìñ Gerando Cap√≠tulo {i + 1}/{number_of_chapters}")
            
            # Determinar qual prompt usar baseado na posi√ß√£o do cap√≠tulo
            if i == 0:
                # Primeiro cap√≠tulo - usar prompt de in√≠cio
                if custom_inicio:
                    chapter_prompt = custom_inicio
                else:
                    chapter_prompt = create_inicio_prompt(title, context, base_prompt, 'medio')
            elif i == number_of_chapters - 1:
                # √öltimo cap√≠tulo - usar prompt de fim
                if custom_fim:
                    chapter_prompt = custom_fim
                else:
                    chapter_prompt = create_final_prompt(title, context, base_prompt, 'medio')
            else:
                # Cap√≠tulos do meio - usar prompt de meio
                if custom_meio:
                    chapter_prompt = custom_meio
                else:
                    chapter_prompt = create_capitulo_prompt(title, context, base_prompt, i, number_of_chapters, 'medio')

            # Gerar o cap√≠tulo
            chapter_content = None
            print(f"üîç DEBUG: Gerando cap√≠tulo {i+1} com prompt de {len(chapter_prompt)} caracteres")

            if ai_provider == 'auto':
                providers = ['openrouter', 'gemini', 'openai']
                for provider in providers:
                    try:
                        print(f"üîç DEBUG: Tentando provider {provider}")
                        if provider == 'openrouter' and api_keys.get('openrouter'):
                            print(f"üîç DEBUG: Chamando OpenRouter...")
                            chapter_content = call_openrouter(chapter_prompt, openrouter_model, api_keys['openrouter'])
                            print(f"‚úÖ DEBUG: OpenRouter retornou {len(chapter_content) if chapter_content else 0} caracteres")
                            break
                        elif provider == 'gemini' and title_generator.gemini_model:
                            print(f"üîç DEBUG: Chamando Gemini...")
                            chapter_content = call_gemini(chapter_prompt, title_generator)
                            print(f"‚úÖ DEBUG: Gemini retornou {len(chapter_content) if chapter_content else 0} caracteres")
                            break
                        elif provider == 'openai' and title_generator.openai_client:
                            print(f"üîç DEBUG: Chamando OpenAI...")
                            chapter_content = call_openai(chapter_prompt, title_generator)
                            print(f"‚úÖ DEBUG: OpenAI retornou {len(chapter_content) if chapter_content else 0} caracteres")
                            break
                        else:
                            print(f"‚ö†Ô∏è DEBUG: Provider {provider} n√£o dispon√≠vel")
                    except Exception as e:
                        print(f"‚ùå Erro com {provider}: {e}")
                        continue
            elif ai_provider == 'openrouter':
                chapter_content = call_openrouter(chapter_prompt, openrouter_model, api_keys['openrouter'])
            elif ai_provider == 'gemini':
                chapter_content = call_gemini(chapter_prompt, title_generator)
            elif ai_provider == 'openai':
                chapter_content = call_openai(chapter_prompt, title_generator)
            
            if chapter_content:
                # Gerar t√≠tulo do cap√≠tulo baseado no conte√∫do
                chapter_title = f"Cap√≠tulo {i + 1}"
                if len(chapter_content) > 50:
                    # Tentar extrair uma frase inicial como t√≠tulo
                    first_sentence = chapter_content.split('.')[0][:50]
                    if len(first_sentence) > 10:
                        chapter_title = f"Cap√≠tulo {i + 1}: {first_sentence}..."

                chapters.append({
                    'chapter_number': i + 1,
                    'title': chapter_title,
                    'content': chapter_content,
                    'word_count': len(chapter_content.split())
                })
                if update_callback:
                    update_callback(chapters)
            else:
                print(f"‚ùå Falha ao gerar cap√≠tulo {i + 1}")
                # Se √© o primeiro cap√≠tulo e falhou, retornar erro
                if i == 0:
                    print(f"‚ùå ERRO CR√çTICO: Falha ao gerar o primeiro cap√≠tulo")
                    return []
                # Se n√£o √© o primeiro, continuar com os cap√≠tulos j√° gerados
                break

            # Pequena pausa entre cap√≠tulos para evitar rate limiting
            time.sleep(1)

        print(f"‚úÖ DEBUG: Gerados {len(chapters)} cap√≠tulos com sucesso")
        return chapters
        
    except Exception as e:
        print(f"‚ùå Erro no Prompt 3: {e}")
        return []

def extract_base_prompt(narrative_structure):
    """Extrair o prompt base da estrutura narrativa"""
    try:
        if isinstance(narrative_structure, str):
            # Tentar parsear como JSON
            import json
            data = json.loads(narrative_structure)
            if 'Prompts' in data and len(data['Prompts']) > 0:
                return data['Prompts'][0].get('prompt', narrative_structure)
        return narrative_structure
    except:
        return narrative_structure

def call_openrouter(prompt, model, api_key):
    """Chamar OpenRouter API"""
    try:
        print(f"üîç DEBUG: Enviando prompt para OpenRouter ({len(prompt)} chars)")
        if model == 'auto':
            model = 'anthropic/claude-3.5-sonnet'
        print(f"üîç DEBUG: Usando modelo: {model}")
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json',
            'HTTP-Referer': 'http://localhost:5173',
            'X-Title': 'Auto Video Producer'
        }
        
        response = requests.post(
            'https://openrouter.ai/api/v1/chat/completions',
            headers=headers,
            json={
                'model': model,
                'messages': [
                    {'role': 'system', 'content': 'Voc√™ √© um especialista em cria√ß√£o de roteiros e storytelling.'},
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 2000,
                'temperature': 0.8
            },
            timeout=60
        )
        
        if response.status_code == 200:
            data = response.json()
            content = data['choices'][0]['message']['content']
            print(f"üîç DEBUG: OpenRouter respondeu com {len(content)} caracteres")
            return content
        else:
            print(f"‚ùå DEBUG: OpenRouter erro {response.status_code}: {response.text}")
            raise Exception(f'OpenRouter API error: {response.status_code}')

    except Exception as e:
        print(f"‚ùå DEBUG: Erro detalhado no OpenRouter: {str(e)}")
        raise Exception(f'Erro OpenRouter: {str(e)}')

def call_gemini(prompt, title_generator=None):
    """Chamar Gemini API com retry autom√°tico entre m√∫ltiplas chaves"""
    import google.generativeai as genai
    from routes.automations import get_next_gemini_key, handle_gemini_429_error
    
    # Tentar m√∫ltiplas chaves se necess√°rio
    max_retries = 3
    last_error = None
    
    for attempt in range(max_retries):
        try:
            # Obter pr√≥xima chave Gemini
            api_key = get_next_gemini_key()
            if not api_key:
                raise Exception('Nenhuma chave Gemini dispon√≠vel. Configure pelo menos uma chave nas Configura√ß√µes.')
            
            print(f"üîç DEBUG: Tentativa {attempt + 1}/{max_retries}: Enviando prompt para Gemini ({len(prompt)} chars)")
            
            # Configurar Gemini diretamente
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            # Gerar conte√∫do
            response = model.generate_content(prompt)
            content = response.text.strip()
            
            print(f"üîç DEBUG: Gemini respondeu com {len(content)} caracteres na tentativa {attempt + 1}")
            return content
            
        except Exception as e:
            error_str = str(e)
            last_error = error_str
            print(f"‚ùå DEBUG: Erro na tentativa {attempt + 1}: {error_str}")
            
            # Check if it's a quota error (429)
            if "429" in error_str or "quota" in error_str.lower() or "rate limit" in error_str.lower():
                if attempt < max_retries - 1:  # Not the last attempt
                    print(f"üîÑ Erro de quota detectado, tentando pr√≥xima chave Gemini...")
                    handle_gemini_429_error(error_str, api_key)
                    continue
                else:
                    print("‚ùå Todas as tentativas de retry falharam")
                    handle_gemini_429_error(error_str, api_key)
            else:
                # For non-quota errors, don't retry
                print(f"‚ùå Erro n√£o relacionado √† quota, parando tentativas: {error_str}")
                break
    
    # Se chegou aqui, todas as tentativas falharam
    final_error = f'Falha na gera√ß√£o com Gemini ap√≥s todas as {max_retries} tentativas. √öltimo erro: {last_error}'
    raise Exception(final_error)

def call_openai(prompt, title_generator):
    """Chamar OpenAI API"""
    try:
        print(f"üîç DEBUG: Enviando prompt para OpenAI ({len(prompt)} chars)")
        response = title_generator.openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Voc√™ √© um especialista em cria√ß√£o de roteiros e storytelling."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2000,
            temperature=0.8
        )
        content = response.choices[0].message.content
        print(f"üîç DEBUG: OpenAI respondeu com {len(content)} caracteres")
        return content
    except Exception as e:
        print(f"‚ùå DEBUG: Erro detalhado no OpenAI: {str(e)}")
        raise Exception(f'Erro OpenAI: {str(e)}')

def generate_long_script(script_data, update_callback=None):
    """Fun√ß√£o principal para gera√ß√£o de roteiros longos - compat√≠vel com pipeline_service.py"""
    try:
        print("üé¨ Iniciando gera√ß√£o de roteiro longo...")
        
        # Extrair dados do script_data
        title = script_data.get('title', '')
        premise = script_data.get('premise', '')
        chapters = script_data.get('chapters', 8)
        style = script_data.get('style', 'inicio')
        duration_target = script_data.get('duration_target', 300)
        include_hooks = script_data.get('include_hooks', True)
        custom_inicio = script_data.get('custom_inicio', '')
        custom_meio = script_data.get('custom_meio', '')
        custom_fim = script_data.get('custom_fim', '')
        
        if not title or not premise:
            return {
                'success': False,
                'error': 'T√≠tulo e premissa s√£o obrigat√≥rios para gera√ß√£o de roteiro'
            }
        
        # Usar diretamente as fun√ß√µes de gera√ß√£o sem Flask request
        print(f"üìù Gerando roteiro: {title}")
        print(f"üìñ Premissa: {premise}")
        print(f"üìä Cap√≠tulos: {chapters}")
        
        # Inicializar o gerador de t√≠tulos para ter acesso √†s APIs
        title_generator = TitleGenerator()
        
        # Configurar APIs automaticamente
        def load_api_keys_from_file():
            """Carrega chaves de API do arquivo JSON"""
            import os
            import json
            try:
                config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'api_keys.json')
                print(f"üîç DEBUG: Caminho do config: {config_path}")
                if os.path.exists(config_path):
                    print("üîç DEBUG: Arquivo de config encontrado")
                    with open(config_path, 'r') as f:
                        keys = json.load(f)
                        print(f"üîç DEBUG: Chaves carregadas: {list(keys.keys())}")
                        return keys
                print("‚ùå DEBUG: Arquivo de config n√£o encontrado")
                return {}
            except Exception as e:
                print(f"‚ùå Erro ao carregar chaves de API: {e}")
                return {}
        
        api_keys = load_api_keys_from_file()
        
        if api_keys.get('openai'):
            print("üîç DEBUG: Configurando OpenAI")
            title_generator.configure_openai(api_keys['openai'])
        else:
            print("‚ùå DEBUG: Chave OpenAI n√£o encontrada")

        gemini_key = api_keys.get('gemini') or api_keys.get('gemini_1')
        if gemini_key:
            print("üîç DEBUG: Configurando Gemini com chave: {gemini_key[:5]}...")
            title_generator.configure_gemini(gemini_key)
        else:
            print("‚ùå DEBUG: Chave Gemini n√£o encontrada")

        if api_keys.get('openrouter'):
            print("üîç DEBUG: Configurando OpenRouter")
            title_generator.configure_openrouter(api_keys['openrouter'])
        else:
            print("‚ùå DEBUG: Chave OpenRouter n√£o encontrada")
        
        # Pipeline de 3 prompts
        print("üé¨ Iniciando pipeline de gera√ß√£o de roteiros...")
        
        # PROMPT 1: Tradu√ß√£o e Contexto
        print("üìù Executando Prompt 1: Tradu√ß√£o e Contexto")
        context_result = execute_prompt_1(title, premise, 'auto', 'auto', api_keys, title_generator)
        
        if not context_result:
            raise Exception("Falha no Prompt 1: Tradu√ß√£o e Contexto")
        
        # PROMPT 2: Estrutura Narrativa
        print("üìñ Executando Prompt 2: Estrutura Narrativa")
        narrative_result = execute_prompt_2(title, context_result, 'auto', 'auto', api_keys, title_generator)
        
        if not narrative_result:
            raise Exception("Falha no Prompt 2: Estrutura Narrativa")
        
        # PROMPT 3: Gera√ß√£o dos Cap√≠tulos
        print(f"‚úçÔ∏è Executando Prompt 3: Gera√ß√£o de {chapters} Cap√≠tulos")
        chapters_list = execute_prompt_3(title, context_result, narrative_result, chapters, 'auto', 'auto', api_keys, title_generator, update_callback, custom_inicio, custom_meio, custom_fim)
        
        if not chapters_list:
            raise Exception("Falha no Prompt 3: Gera√ß√£o dos Cap√≠tulos")
        
        # Combinar todos os cap√≠tulos em um roteiro √∫nico
        full_script = ""
        for chapter in chapters_list:
            full_script += f"\n\n{chapter.get('title', '')}\n\n"
            full_script += chapter.get('content', '')
        
        # Calcular dura√ß√£o estimada (aproximadamente 150 palavras por minuto)
        word_count = len(full_script.split())
        # Garantir que duration_target seja um n√∫mero - extrair apenas n√∫meros
        if isinstance(duration_target, str):
            # Extrair n√∫meros da string (ex: "5-7 minutes" -> 5)
            import re
            numbers = re.findall(r'\d+', duration_target)
            duration_target_num = int(numbers[0]) if numbers else 5
        else:
            duration_target_num = duration_target
        estimated_duration = max(word_count / 150, duration_target_num / 60)  # em minutos
        
        print(f"‚úÖ Roteiro gerado com sucesso: {len(chapters_list)} cap√≠tulos, {word_count} palavras")
        
        return {
            'success': True,
            'data': {
                'script': full_script.strip(),
                'chapters': chapters_list,
                'estimated_duration': estimated_duration,
                'word_count': word_count,
                'style': style,
                'provider_used': 'auto',
                'context': context_result,
                'narrative_structure': narrative_result
            }
        }
                
    except Exception as e:
        print(f"‚ùå Erro na gera√ß√£o de roteiro longo: {e}")
        return {
            'success': False,
            'error': str(e)
        }
